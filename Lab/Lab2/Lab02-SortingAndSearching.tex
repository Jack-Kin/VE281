\documentclass[12pt,a4paper]{article}
%\usepackage{ctex}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{enumitem,balance}
\usepackage{wrapfig}
\usepackage{mathrsfs,euscript}
\usepackage[x11names,svgnames,dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage[vlined,ruled,commentsnumbered,linesnumbered]{algorithm2e}
\usepackage{listings}
\usepackage{multicol}


\usepackage{algorithmic}
%\usepackage{fontspec}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm



\renewcommand{\listalgorithmcfname}{List of Algorithms}
\renewcommand{\algorithmcfname}{Alg.}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{exercise}{Exercise}
\newtheorem*{solution}{Solution}
\newtheorem{definition}{Definition}
\theoremstyle{definition}


%\numberwithin{equation}{section}
%\numberwithin{figure}{section}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\postscript}[2]
 {\setlength{\epsfxsize}{#2\hsize}
  \centerline{\epsfbox{#1}}}

\renewcommand{\baselinestretch}{1.0}

\setlength{\oddsidemargin}{-0.365in}
\setlength{\evensidemargin}{-0.365in}
\setlength{\topmargin}{-0.3in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{10.1in}
\setlength{\textwidth}{7in}
\makeatletter \renewenvironment{proof}[1][Proof] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother
\makeatletter
\renewenvironment{solution}[1][Solution] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother


\definecolor{codegreen}{rgb}{0.44,0.68,0.28}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstset{
language=C++,
frame=shadowbox,
keywordstyle = \color{blue}\bfseries,
commentstyle=\color{codegreen},
tabsize = 4,
backgroundcolor=\color{backcolour},
numbers=left,
numbersep=5pt,
breaklines=true,
emph = {int,float,double,char},emphstyle=\color{orange},
emph ={[2]const, typedef},emphstyle = {[2]\color{red}} }



\begin{document}
\noindent

%========================================================================
\noindent\framebox[\linewidth]{\shortstack[c]{
\Large{\textbf{Lab02-Sorting and Searching}}\vspace{1mm}\\
VE281 - Data Structures and Algorithms, Xiaofeng Gao, TA: Li Ma, Autumn 2019}}
%CS26019 - Algorithm Design and Analysis, Xiaofeng Gao, Autumn 2019}}
\begin{center}
\footnotesize{\color{red}$*$ Please upload your assignment to website. Contact webmaster for any questions.}

\footnotesize{\color{blue}$*$ Name:Jin Zhejian	\quad Student ID: 517370910167 \quad Email: jinzhejian@outlook.com}
\end{center}


\begin{enumerate}

\item \textbf{Cocktail Sort.} Consider the pseudo code of a sorting algorithm shown in Alg.~\ref{Alg-Cocktail}, which is called \emph{Cocktail Sort}, then answer the following questions.


\begin{minipage}[t]{0.4\textwidth}
\begin{enumerate}
\item What is the minimum number of element comparisons performed by the algorithm? When is this minimum achieved?
\item What is the maximum number of element comparisons performed by the algorithm? When is this maximum achieved?
\item Express the running time of the algorithm in terms of the $O$ notation.
\item Can the running time of the algorithm be expressed in terms of the $\Theta$ notation? Explain.
\end{enumerate}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.5\textwidth}
\begin{algorithm}[H]
		\caption{CocktailSort($a$[$\cdot$], $n$)} \label{Alg-Cocktail}
		\KwIn {an array $a$, the length of array $n$}
		\For {$i=0; i<n-1; i++$}
		{
			$bFlag \leftarrow true$;
			
			\For {$j=i;j<n-i-1;j++$}
			{
				\If {$a[j]>a[j+1]$}
				{
					swap($a[j]$, $a[j+1]$)\;
					$bFlag \leftarrow false$;
				}
			}
			
			\If {bFlag}
			{
				break;
			}
			
			$bFlag \leftarrow true$;			
			
			\For {$j=n-i-1;j>i;j--$}
			{
				\If {$a[j]<a[j-1]$}
				{
					swap($a[j]$, $a[j-1]$)\;
					$bFlag \leftarrow false$;
				}
			}
			\If {bFlag}
			{
				break;
			}
		}
\end{algorithm}
\end{minipage}

%\end{multicols}
\begin{solution}
	~\\
	(a) The minimum number is $n-1$. If the original array is in ascending order, the number of element comparisons is the minimum one.
	It only need to go through the inner \textbf{for loop}, which needs $n-1$ comparisons, and  \textbf{bFlag} is true after the for loop, then the \textbf{outer for loop} breaks and the Cocktail Sort is done.
	
	(b) The maximum number of element comparisons is $\lfloor \frac{n^2}{2}\rfloor$, which means $\frac{n^2-1}{2}$ when n is odd, $\frac{n^2}{2}$ when n is even. This maximum achieves when the original array is in decreasing order. In this case, when n is odd, for each i, $n-2i-1$ times of comparisons are needed. So, in total, there needs $(n-1) + (n-1) + (n-3) + (n-3) + ...... + (2) + (2) = 2\cdot \frac{(n-1+2)\frac{n-1}{2}}{2} = \frac{(n+1)(n-1)}{2} = \frac{n^2-1}{2}$.  When n is even, similarly, there needs $(n-1) + (n-1) + ...... + (3) + (3) + (1) + (1) = \frac{n^2}{2}.$
	
	(c) For the worst case, there needs $\lfloor \frac{n^2}{2}\rfloor$ element comparisons, and swap function don't need element comparisons. Therefore, the time complexity of Cocktail Sort is $O(n^2)$.
	
	(d) No. Because the time complexity for the best case is $\Omega(n)$, which is not of the same order with $n^2$. Therefore, the running time cannot be expressed in terms of the $\Theta$ notation.
\end{solution}


\newpage

\item \textbf{In-Place.} In place means an algorithm requires $O(1)$ additional memory, including the stack space used in recursive calls. Frankly speaking, even for a same algorithm, different implementation methods bring different in-place characteristics. Taking \emph{Binary Search} as an example, we give two kinds of implementation pseudo codes shown in Alg.~\ref{Alg-RecursiveBS} and Alg.~\ref{Alg-NonRecursiveBS}. Please analyze whether they are in place.
    
    Next, please give one similar example regarding other algorithms you know to illustrate such phenomenon.

%\item  \textbf{Master Theorem}.
%
%\begin{definition}[Matrix Multiplication]
%The product of two $n \times n$ matrices $X$ and $Y$ is a third $n \times n$ matrix $Z = XY$, with $(i,j)$th entry
%$$Z_{ij}=\sum_{k=1}^{n}X_{ik}Y_{kj}.$$
%\end{definition}
%$Z_{ij}$ is the dot product of the $i$th row of $X$ with $j$th column of $Y$. The preceding formula implies an $O(n^3)$ algorithm for matrix multiplication.

\begin{minipage}[t]{0.49\textwidth}
\begin{algorithm}[H]
	\BlankLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\caption{BinSearch($a[\cdot]$, $x$, $low$, $high$)} \label{Alg-RecursiveBS}
	\Input{a sorted array $a$ of $n$ elements, an integer $x$, first index $low$, last index $high$}
	\Output{first index of key $x$ in $a$, $-1$ if not found}
	\BlankLine
	\If{$high < low$}{
	\Return{-1};
	}
    $mid \leftarrow low +((high - low) / 2)$;
    
    
    \uIf{$a[mid] > x$}{
	$mid \leftarrow$ {$\text{BinSearch}(a,x,low, mid - 1)$};
	}
    \uElseIf{$a[mid] < x$}{
    $mid \leftarrow$ {$\text{BinSearch}(a,x,mid+1, high)$};
    }
   \Else {
    \Return{$mid$};
    }
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.455\textwidth}
\begin{algorithm}[H]
\BlankLine
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\caption{BinSearch($a[\cdot]$, $x$, $low$, $high$)} \label{Alg-NonRecursiveBS}
	\Input{a sorted array $a$ of $n$ elements, an integer $x$, first index $low$, last index $high$}
	\Output{first index of key $x$ in $a$, $-1$ if not found}
	\BlankLine	
	\While{$low \leq high$}{
	$mid \leftarrow low + ((high - low) / 2)$;
	
	\uIf{$a[mid] > x$}
	{
		$high \leftarrow mid - 1$;
	}
	\uElseIf{$a[mid] < x$}{
	    $low \leftarrow mid + 1$;
	}
	\Else{
	    \Return{$mid$};
	}
	}
	\Return{-1};
\end{algorithm}\end{minipage}
\begin{solution}
~\\
\textbf{Alg.2} takes recursion method. In each recursion, the algorithm needs to create an int space to store $\textbf{\textit{mid}}$ , and it needs around $\log n$ times of recursion. So, \textbf{Alg.2} is not in place, its space complexity is $O(\log n)$. 

\textbf{Alg.3} takes traversal method. It only needs to require $O(1)$ additional memory to save $\textbf{\textit{mid}}$, and in every loop, $\textbf{\textit{mid}}$ is replaced by new $\textbf{\textit{mid}}$. So, \textbf{Alg.3} is in place.

Similar example:


\begin{minipage}[t]{0.45\textwidth}
	
	\begin{algorithm}[H]
		\caption{SumArray(a[$\cdot$], n)}
		\label{alg:3}
		\begin{algorithmic}
			\REQUIRE an array with n numbers
			\ENSURE get the sum of each numbers \\ \qquad \quad \, \,  in the array
			\BlankLine	
			\IF{n = 0}
			\STATE return 0;
			\ELSE
			\RETURN SumArray(a[$\cdot$], n-1) + a[n -1];
			\ENDIF

		\end{algorithmic}
	\end{algorithm}

\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
	\begin{algorithm}[H]
	\caption{SumArray(a[$\cdot$], n)}
	\label{alg:4}
	\begin{algorithmic}
		\REQUIRE an array with n numbers
		\ENSURE get the sum of each numbers \\ \qquad \quad \, \,  in the array
		\BlankLine	
		\STATE sum $\leftarrow$ 0;
		\FOR{each $i \in [0 , n - 1]$}
		\STATE sum $\leftarrow$ sum + a[i];
		\ENDFOR
		\RETURN sum;
	\end{algorithmic}
\end{algorithm}
\end{minipage}

\textbf{Alg.4} uses recursive method, its space complexity is $O(n)$ ,while \textbf{Alg.5} uses traversal method, which is in place.

\end{solution}


\newpage

\item  \textbf{Master Theorem}.

\begin{definition}[Matrix Multiplication]
	The product of two $n \times n$ matrices $X$ and $Y$ is a third $n \times n$ matrix $Z = XY$, with $(i,j)$th entry
	$$Z_{ij}=\sum_{k=1}^{n}X_{ik}Y_{kj}.$$
\end{definition}
$Z_{ij}$ is the dot product of the $i$th row of $X$ with $j$th column of $Y$. The preceding formula implies an $O(n^3)$ algorithm for matrix multiplication.


In 1969, the German mathematician Volker Strassen announced a siginificantly more efficient algorithm, based upon divide-and-conquer. Matrix Multiplication can be performed blockwise. To see what this means, carve $X$ into four $\frac{n}{2} \times \frac{n}{2}$ blocks, and also $Y$:
\begin{displaymath}
X=
\left(\begin{array}{c|c}
A & B \\
\hline
C & D \end{array}\right), \quad
Y=\left(\begin{array}{c|c}
E & F \\
\hline
G & H \end{array}\right).
 \end{displaymath}

Then their product can be expressed in terms of these blocks and is exactly as if the blocks were single elements.

 \begin{displaymath}
 XY=
\left(\begin{array}{c|c}
A & B \\
\hline
C & D \end{array}\right)
\left(\begin{array}{c|c}
E & F \\
\hline
G & H \end{array}\right)
=
\left(\begin{array}{c|c}
AE+BG & AF+BH \\
\hline
CE+DG & CF+DH \end{array}\right).
 \end{displaymath}

To compute the size-$n$ product $XY$, recursively compute eight size-$\frac{n}{2}$ products $AE$,  $BG$, $AF$, $BH$, $CE$, $DG$, $CF$, $DH$ and then do a few additions.

\begin{enumerate}
\item Write down the recurrence function of the above method and compute its running time by Master Theorem.

\begin{solution}
~\\
For each blockwise, we need to calculate 8 times of multiplications with size-$\frac{n}{2}$ and 4 additions of size-$\frac{n}{2}$ matrices. 
Therefore, the recurrence function is: 
$$
T(n) = 8 \cdot T(\frac{n}{2}) + 4 \cdot {(\frac{n}{2})^2} =  8 \cdot T(\frac{n}{2}) + n^2
$$
$a = 8, b = 2, d = 2, a > b^d$, so by Master Theorem:
$$
T(n) = O(n^{\log_b a}) = O(n^3)
$$

\end{solution}

\item The efficiency can be further improved. It turns out $XY$ can be computed from just seven $\frac{n}{2}\times \frac{n}{2}$ sub problems.

\begin{displaymath}
 XY=
\left(\begin{array}{c|c}
P_{5}+P_{4}-P_{2}+P_{6} & P_{1}+P_{2} \\
\hline
P_{3}+P_{4} & P_{1}+P_{5}-P_{3}-P_{7} \end{array}\right),
\end{displaymath}
where
\begin{align*}
P_{1}&=A(F-H), \qquad P_{2}=(A+B)H, \qquad P_{3}=(C+D)E, \qquad P_{4}=D(G-E),\\
P_{5}&=(A+D)(E+H),\qquad P_{6}=(B-D)(G+H),\qquad P_{7}=(A-C)(E+H).
\end{align*}

Write the corresponding recurrence function and compute the new running time.

\begin{solution}
~\\
For each blockwise, we need to calculate 7  times of multiplications with size-$\frac{n}{2}$ and 10 additions of size-$\frac{n}{2}$ matrices. 
Therefore, the recurrence function is: 
$$
T(n) = 7 \cdot T(\frac{n}{2}) + 10 \cdot {(\frac{n}{2})^2} = 7 \cdot T(\frac{n}{2}) + \frac{5}{2} \cdot n^2
$$
$a = 7, b = 2, d = 2, a > b^d$, so by Master Theorem:
$$
T(n) = O(n^{\log_b a}) = O(n^{\log_2 7})
$$
\end{solution}

\end{enumerate}


\end{enumerate}

%========================================================================
\end{document}
